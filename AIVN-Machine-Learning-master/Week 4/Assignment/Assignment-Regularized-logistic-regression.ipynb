{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment: Regularized Logistic Regression\n",
    "\n",
    "Chào mừng các bạn đến với bài tập lập trình Regularized Logistic Regression (Bài toán phân loại nhị phân - 2 nhóm). Trước khi thực hiện bài tập này, các bạn nên học kỹ các kiến thức lý thuyết. Nếu có bất kỳ câu hỏi hay vấn đề nào xảy ra, các bạn hãy để lại comment trực tiếp bên dưới bài đăng hoặc liên hệ qua Fanpage AIVIETNAM.\n",
    "\n",
    "### Hướng dẫn làm bài \n",
    "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
    "- Cố gắng không sử dụng các vòng lặp (for, while). \n",
    "- Hãy sử dụng các hàm của thư viện numpy.\n",
    "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới. \n",
    "\n",
    "Các bạn sẽ bắt đầu Code trong phần `### START CODE HERE ###` và `### END CODE HERE ###`. Các bạn nhớ đừng sửa bất kỳ dòng Code nào bên ngoài những câu lệnh này. \n",
    "\n",
    "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó. \n",
    "\n",
    "Trong phần Code: các bạn hãy cố gắng thực hiện ít dòng Code nhất theo chỉ định \"(≈ X lines of code)\". Mặc dù đây không phải là hạn chế về số dòng Code của bạn, nhưng hãy tối ưu sao cho ít nhất có thể.\n",
    "\n",
    "### Chú ý\n",
    "\n",
    "Trong phần bài tập này, chúng ta sẽ sử dụng **Advanced Optimization**\n",
    "\n",
    "Ưu điểm của phương pháp này:\n",
    "- Không cần phải chọn **learning_rate** (có một vòng lặp bên trong thuật toán để tìm ra giá trị learning_rate tốt nhất).\n",
    "- Thường hội tụ nhanh hơn Gradient Descent.\n",
    "- Dễ dàng sử dụng mà không cần tìm hiểu quá sâu bên trong.\n",
    "\n",
    "Nhược điểm:\n",
    "- Không nên tự thực hiện mà hãy sử dụng thư viện có sẵn.\n",
    "- Nếu có lỗi xảy ra việc sửa lỗi thường phức tạp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện \n",
    "# Standard imports. Importing seaborn for styling.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "\n",
    "Trong phần bài tập này, Regularized Logistic Regression sẽ được sử dụng để dự đoán các vi mạch chế tạo từ nhà máy có vượt qua kiểm duyệt hay không?\n",
    "\n",
    "### 1. Trực quan hoá dữ liệu \n",
    "\n",
    "Như trong phần bài tập Logistic Regression, chúng ta sẽ nhập dữ liệu và trực quan hoá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data2.txt', delimiter=',')\n",
    "X, y = data[:,:2], data[:,2]\n",
    "\n",
    "# Viewing the imported values (first 5 rows)\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trực quan dữ liệu \n",
    "# Creating plotData method to display the figure where the axes are the two exam scores.\n",
    "def plotData(x, y, xlabel, ylabel, labelPos, labelNeg):\n",
    "    \n",
    "    # Separating positive and negative scores (in this case 1 and 0 values):\n",
    "    pos = y==1\n",
    "    neg = y==0\n",
    "\n",
    "    # Scatter plotting the data, filtering them according the pos/neg values:\n",
    "    plt.scatter(x[pos, 0], x[pos, 1], s=30, c='darkblue', marker='+', label=labelPos)\n",
    "    plt.scatter(x[neg, 0], x[neg, 1], s=30, c='yellow', marker='o', edgecolors='y', label=labelNeg)\n",
    "\n",
    "    # Labels and limits:\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(x[:, 0].min(), x[:, 0].max())\n",
    "    plt.ylim(x[:, 1].min(), x[:, 1].max())\n",
    "\n",
    "    # Legend:\n",
    "    pst = plt.legend(loc='upper right', frameon=True)\n",
    "    pst.get_frame().set_edgecolor('k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accepted: Được chấp nhận.\n",
    "Rejected: Không được chấp nhận.\n",
    "\"\"\"\n",
    "\n",
    "plotData(X, y, 'Microchip Test 1', 'Microchip Test 2', 'Accepted', 'Rejected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature mapping\n",
    "\n",
    "Trong phần này, chúng ta sẽ sử dụng Polynomial Logistic Regression để có thể tìm được decision boundary phân cách 2 nhóm dữ liệu. Hàm __PolynomialFeatures__ từ thư viện __scikit-learn__ sẽ giúp chúng ta thực hiện.\n",
    "\n",
    "Ví dụ về cách sử dụng __PolynomialFeatures__ đã được đề cập trong phần bài toán Polynomial Linear Regression, xem lại [tại đây](https://nbviewer.jupyter.org/github/thanhhff/AIVN-Machine-Learning/blob/master/Week%203/Polynomial-regression.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PolynomialFeatures\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Creating the model\n",
    "# Đa thức bậc 6\n",
    "poly = PolynomialFeatures(6)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành đa thức bậc 6\n",
    "X2 = poly.fit_transform(X)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost function và Gradient\n",
    "\n",
    "Trong phần này bạn sẽ viết hàm __cost function__ và __gradient methods__ cho Regularized logistic regression.\n",
    "\n",
    "#### 1. Cost Function \n",
    "\n",
    "**Bài tập:** Viết hàm Cost.\n",
    "\n",
    "> $h = g(X\\theta)$ \n",
    "\n",
    "**Chú ý:** do lỗi làm tròn số nên giá trị khi tính `Sigmoid` khiến `log(0)` không xác định. Nên để loại bỏ lỗi này ta thêm giá trị `eps = 1e-15` (một số nhỏ vừa đủ) vào công thức `J`:\n",
    "\n",
    "> $J(\\theta) = - \\frac{1}{m} [ y^T \\log{h} + (1-y)^T\\log{(1 - h + eps)} ] + \\frac{\\lambda}{2m} \\sum_{j = 1}^{n} \\theta_j^2$\n",
    "\n",
    "**Chú ý:** không đánh phạt hệ số $\\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "# Sử dụng hàm sigmoid trong thư viện scipy \n",
    "def sigmoid(z):\n",
    "    return expit(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm regularized costFunction:\n",
    "def costFunctionR(theta, X, y, lam):\n",
    "    # Số lượng training\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    m = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    eps = 1e-15\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 line of code)\n",
    "    hThetaX = None\n",
    "    J = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gradient methods \n",
    "\n",
    "**Bài tập:** Viết Gradient Methods \n",
    "\n",
    "> $gradient = \\frac{1}{m} X^T (g(X\\theta) - \\vec{y}) + \\lambda\\theta' = \\frac{1}{m} X^T (h - \\vec{y}) + \\lambda\\theta'$\n",
    "\n",
    "Trong đó:\n",
    "- $\\theta'$ có giá trị $\\theta_0 = 0$ (vì không đánh phạt $\\theta_0$), các giá trị còn lại tương tự với $\\theta$.\n",
    "\n",
    "**Gợi ý:** hàm [numpy.insert](https://docs.scipy.org/doc/numpy/reference/generated/numpy.insert.html)\n",
    "\n",
    "``` \n",
    "numpy.insert(arr, obj, values, axis=None)\n",
    "\n",
    "arr: input_array \n",
    "obj: vị trí cần chèn \n",
    "values: giá trị \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm regularized gradientFunc:\n",
    "def gradientFuncR(theta, X, y, lam):\n",
    "    ### START CODE HERE ### (≈ 4 line of code)\n",
    "    \n",
    "    m = None\n",
    "    hThetaX = None\n",
    "    \n",
    "    # Không regularizing tham số θ0 nên sẽ thay θ0 = 0  \n",
    "    thetaNoZeroReg = None\n",
    "    \n",
    "    gradient = None \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo theta\n",
    "initial_theta = np.zeros(X2.shape[1])\n",
    "initial_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gọi hàm __*costFunctionR*__ và __*gradientFuncR*__ sử dụng tham số θ được khởi tạo ở trên, với giá trị lambda = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 2 line of code)\n",
    "J = None\n",
    "gradient = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Giá trị Cost khoảng 0.693 cho phần này\n",
    "print(\"Cost: %0.3f\"%(J))\n",
    "print(\"Gradient: {0}\".format(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "Cost: 0.693\n",
    "Gradient: [8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02\n",
    " 1.15013308e-02 3.76648474e-02 1.83559872e-02 7.32393391e-03\n",
    " 8.19244468e-03 2.34764889e-02 3.93486234e-02 2.23923907e-03\n",
    " 1.28600503e-02 3.09593720e-03 3.93028171e-02 1.99707467e-02\n",
    " 4.32983232e-03 3.38643902e-03 5.83822078e-03 4.47629067e-03\n",
    " 3.10079849e-02 3.10312442e-02 1.09740238e-03 6.31570797e-03\n",
    " 4.08503006e-04 7.26504316e-03 1.37646175e-03 3.87936363e-02]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tìm tham số theta bằng scipy.optimize sử dụng .minimize\n",
    "\n",
    "Thực hiện tìm tham số theta bằng __scipy.optimize__ sử dụng __*.minimize__, như trong phần bài tập __2.3__ (Logistic Regression).\n",
    "\n",
    "```\n",
    "scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)[source]\n",
    "```\n",
    "\n",
    "Các bạn sẽ cài đặt một số tham số dưới đây (ngoài ra không cần thiết).\n",
    "\n",
    "```\n",
    "- fun: costFunctionR\n",
    "- x0: initial_theta\n",
    "- args: (X2, y, 1) - vì trong bài này sử dụng Regularized nên đối số 1 dành cho lambda\n",
    "- method: trong bài này chúng ta sử dụng BFGS\n",
    "- jac: gradientFuncR \n",
    "- options={'maxiter' : 400, 'disp': True} với maxiter: số lần lặp tối đa; disp: hiển thị thông tin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing minimize from scipy:\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "### START CODE HERE ### (≈ 2 line of code)\n",
    "result2 = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "result2['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "```\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.529003\n",
    "         Iterations: 47\n",
    "         Function evaluations: 48\n",
    "         Gradient evaluations: 48\n",
    "array([ 1.27268739,  0.62557016,  1.1809665 , -2.01919822, -0.91761468,\n",
    "       -1.43194199,  0.12375921, -0.36513086, -0.35703388, -0.17485805,\n",
    "       -1.45843772, -0.05129676, -0.61603963, -0.2746414 , -1.19282569,\n",
    "       -0.24270336, -0.20570022, -0.04499768, -0.27782709, -0.29525851,\n",
    "       -0.45613294, -1.04377851,  0.02762813, -0.29265642,  0.01543393,\n",
    "       -0.32759318, -0.14389199, -0.92460119])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vẽ decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(X, y, title):\n",
    "    # Plot the data\n",
    "    plotData(X[:, 1:3], y, 'Microchip Test 1', 'Microchip Test 2', 'Accepted', 'Rejected')\n",
    "    \n",
    "    # Defining the data to use in the meshgrid calculation. Outputting xx and yy ndarrays\n",
    "    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = sigmoid(poly.fit_transform(np.c_[xx.ravel(), yy.ravel()]).dot(result2['x']))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plotting the contour plot\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=1, colors='g')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Thay đổi giá trị của λ\n",
    "\n",
    "Trong phần bài tập này, các bạn sẽ thấy nếu với λ khác nhau trong thì decision boundary sẽ thay đổi như thế nào. Chúng ta sẽ bị overfitting nếu không sử dụng regularization tức λ = 0. Mặt khác, nếu λ quá lớn sẽ bị underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 15))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Creating 3 subplots using 3 different λ values\n",
    "for i, lam in enumerate([0, 1, 100]):\n",
    "    result2 = minimize(costFunctionR, initial_theta, args=(X2, y, lam), method='BFGS', jac=gradientFuncR, \n",
    "                       options={'maxiter' : 400, 'disp': False})\n",
    "    \n",
    "    if (lam == 0):\n",
    "        title = 'No regularization (Overfitting) (λ = 0)'\n",
    "    elif (lam == 100):\n",
    "        title = 'Too much regularization (Underfitting) (λ = 100)'\n",
    "    else:\n",
    "        title = 'Training data with decision boundary (λ = 1)'\n",
    "    \n",
    "    plt.subplot(3, 1, i+1)\n",
    "    \n",
    "    # Plotting the decision boundary plot\n",
    "    plotDecisionBoundary(X2, y, title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tổng kết\n",
    "\n",
    "Thông qua bài tập này, các bạn đã nắm vững các kiến thức về:\n",
    "\n",
    "- Regularized Logistic Regression\n",
    "- Triển khai hàm Cost Function và Gradient Method sử dụng BFGS\n",
    "- Đánh giá sự khác biệt khi sử dụng Regularized.\n",
    "- Thay đổi giá trị của lambda khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo\n",
    "\n",
    "[1] [CS229 - Machine Learning]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
