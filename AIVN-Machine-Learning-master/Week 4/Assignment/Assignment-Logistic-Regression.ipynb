{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment: Logistic Regression\n",
    "\n",
    "Chào mừng các bạn đến với bài tập lập trình Logistic Regression (Bài toán phân loại nhị phân - 2 nhóm). Trước khi thực hiện bài tập này, các bạn nên học kỹ các kiến thức lý thuyết. Nếu có bất kỳ câu hỏi hay vấn đề nào xảy ra, các bạn hãy để lại comment trực tiếp bên dưới bài đăng hoặc liên hệ qua Fanpage AIVIETNAM.\n",
    "\n",
    "### Hướng dẫn làm bài \n",
    "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
    "- Cố gắng không sử dụng các vòng lặp (for, while). \n",
    "- Hãy sử dụng các hàm của thư viện numpy.\n",
    "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới. \n",
    "\n",
    "Các bạn sẽ bắt đầu Code trong phần `### START CODE HERE ###` và `### END CODE HERE ###`. Các bạn nhớ đừng sửa bất kỳ dòng Code nào bên ngoài những câu lệnh này. \n",
    "\n",
    "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó. \n",
    "\n",
    "Trong phần Code: các bạn hãy cố gắng thực hiện ít dòng Code nhất theo chỉ định \"(≈ X lines of code)\". Mặc dù đây không phải là hạn chế về số dòng Code của bạn, nhưng hãy tối ưu sao cho ít nhất có thể.\n",
    "\n",
    "### Chú ý\n",
    "\n",
    "Trong phần bài tập này, chúng ta sẽ sử dụng **Advanced Optimization**\n",
    "\n",
    "Ưu điểm của phương pháp này:\n",
    "- Không cần phải chọn **learning_rate** (có một vòng lặp bên trong thuật toán để tìm ra giá trị learning_rate tốt nhất).\n",
    "- Thường hội tụ nhanh hơn Gradient Descent.\n",
    "- Dễ dàng sử dụng mà không cần tìm hiểu quá sâu bên trong.\n",
    "\n",
    "Nhược điểm:\n",
    "- Không nên tự thực hiện mà hãy sử dụng thư viện có sẵn.\n",
    "- Nếu có lỗi xảy ra việc sửa lỗi thường phức tạp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện \n",
    "# Standard imports. Importing seaborn for styling.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "Nhiệm vụ của bài tập này là dự đoán thí sinh có được vào trường đại học hay không từ điểm số của 2 kỳ thi.\n",
    "\n",
    "### 1. Visualizing the data (Trực quan hoá dữ liệu)\n",
    "\n",
    "Nhập và vẽ dữ liệu đã cho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data. 2 cột đầu tiên chứa điểm thi và cột thứ 3 chứa nhãn.\n",
    "data = np.loadtxt('data/data1.txt', delimiter=',')\n",
    "X, y = data[:,:2], data[:,2]\n",
    "\n",
    "# Viewing the imported values (first 5 rows)\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trực quan dữ liệu \n",
    "# Creating plotData method to display the figure where the axes are the two exam scores.\n",
    "def plotData(x, y, xlabel, ylabel, labelPos, labelNeg):\n",
    "    \n",
    "    # Separating positive and negative scores (in this case 1 and 0 values):\n",
    "    pos = y==1\n",
    "    neg = y==0\n",
    "\n",
    "    # Scatter plotting the data, filtering them according the pos/neg values:\n",
    "    plt.scatter(x[pos, 0], x[pos, 1], s=30, c='darkblue', marker='+', label=labelPos)\n",
    "    plt.scatter(x[neg, 0], x[neg, 1], s=30, c='yellow', marker='o', edgecolors='y', label=labelNeg)\n",
    "\n",
    "    # Labels and limits:\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(x[:, 0].min(), x[:, 0].max())\n",
    "    plt.ylim(x[:, 1].min(), x[:, 1].max())\n",
    "\n",
    "    # Legend:\n",
    "    pst = plt.legend(loc='upper right', frameon=True)\n",
    "    pst.get_frame().set_edgecolor('k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the initial figure:\n",
    "plotData(X, y, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not Admitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Thực hiện bài toán \n",
    "\n",
    "#### 2.1 Sigmoid Function\n",
    "\n",
    "**Bài tập:** Viết hàm Sigmoid.\n",
    "\n",
    "Hàm giả thuyết của Logistic Regression được định nghĩa:\n",
    "$h_\\theta(x) = g(\\theta^{T}x)$,\n",
    "trong đó g là sigmoid function: <br><br>\n",
    "\n",
    "<center>$g(z) = \\frac{1}{1 +  e^{-z}}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm Sigmoid \n",
    "\n",
    "def sigmoid(z):\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    g = None\n",
    "    ### END CODE HERE ###\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện vẽ hàm Sigmoid \n",
    "\n",
    "x_val = np.linspace(-10, 10, 10000)\n",
    "\n",
    "# and plotting the calculated sigmoid function:\n",
    "plt.plot(x_val, sigmoid(x_val))\n",
    "\n",
    "# Labels and limits\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.xlim(x_val.min(), x_val.max())\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cost function and gradient\n",
    "\n",
    "Trong phần này bạn sẽ viết __cost function__ và __gradient methods__ cho logistic regression.\n",
    "\n",
    "##### 1. Cost Function \n",
    "\n",
    "**Bài tập:** Viết hàm Cost.\n",
    "\n",
    "> $h = g(X\\theta)$ \n",
    "\n",
    "**Chú ý:** do lỗi làm tròn số nên giá trị khi tính `Sigmoid` khiến `log(0)` không xác định. Nên để loại bỏ lỗi này ta thêm giá trị `eps = 1e-15` (giá trị nhỏ vừa đủ) vào công thức `J`:\n",
    "\n",
    "> $J(\\theta) = - \\frac{1}{m}[ y^T \\log{h} + (1-y)^T\\log{(1 - h + \\text{eps}}) ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm CostFunction:\n",
    "def costFunction(theta, X, y):\n",
    "    # Số lượng training \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    m = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # eps = 1e-15 \n",
    "    # Khắc phục lỗi không xác định khi thực hiến tối thiểu hoá sử dụng BFGS minimization\n",
    "    eps = 1e-15\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 line of code)\n",
    "    hThetaX = None\n",
    "    J = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Gradient methods\n",
    "\n",
    "**Bài tập:** Viết Gradient Methods \n",
    "\n",
    "> $gradient = \\frac{1}{m} X^T (g(X\\theta) - \\vec{y}) = \\frac{1}{m} X^T (h - \\vec{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết hàm gradientFunc:\n",
    "\n",
    "def gradientFunc(theta, X, y):\n",
    "    # Số lượng training \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    m = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 line of code)\n",
    "    hThetaX = None\n",
    "    gradient =  None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập:** Thêm giá trị 1 vào X. Và đồng thời khởi tạo theta có giá trị 0. (Tương tự trong bài toán hồi quy tuyến tính sử dụng Vector hoá)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 2 line of code)\n",
    "X = None\n",
    "theta = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "array([0., 0., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gọi hàm __*costFunction*__ và __*gradientFunc*__ sử dụng tham số vừa khởi tạo ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 2 line of code)\n",
    "J = None\n",
    "gradient = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Giá trị cost nên là: 0.693 cho phần này \n",
    "print(\"Cost: %0.3f\"%(J))\n",
    "print(\"Gradient: {0}\".format(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "Cost: 0.693\n",
    "Gradient: [ -0.1        -12.00921659 -11.26284221]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tìm tham số theta bằng scipy.optimize sử dụng .minimize\n",
    "\n",
    "Trong phần bài tập này, thay vì dùng Gradient Descent thông thường như trong các bài tập về Linear Regression. Chúng ta sẽ dùng thư viện scipy \n",
    "[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) với __[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)__ để tìm giá trị hội tụ cho bài toán Logistic Regression. \n",
    "\n",
    "```\n",
    "scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)[source]\n",
    "```\n",
    "\n",
    "Các bạn sẽ cài đặt một số tham số dưới đây (ngoài ra không cần thiết).\n",
    "\n",
    "```\n",
    "- fun: costFunction\n",
    "- x0: theta \n",
    "- args=(X,y)\n",
    "- method: trong bài này chúng ta sử dụng BFGS\n",
    "- jac: gradientFunc \n",
    "- options={'maxiter' : 400, 'disp': True} với maxiter: số lần lặp tối đa; disp: hiển thị thông tin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing minimize from scipy:\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Tìm kiếm giá trị tốt nhất cho θ \n",
    "# Giá trị của Cost khoảng: 0.203 cho bài toán này \n",
    "\n",
    "### START CODE HERE ### (≈ 2 line of code)\n",
    "result = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.203498\n",
    "         Iterations: 20\n",
    "         Function evaluations: 27\n",
    "         Gradient evaluations: 27\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Đánh giá logistic regression\n",
    "\n",
    "Sau khi thực hiện tính toán tìm các tham số, chúng ta có thể sử dụng mô hình này để xem các sinh viên nào sẽ đỗ.\n",
    "Đồng thời chúng ta cũng vẽ Decision Boundary (đường ranh giới) trong hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giá trị của θ sau khi thực hiện hội tụ ở trên\n",
    "gradBFGS = result['x']\n",
    "\n",
    "# Tính x và y cho Decision Boundary \n",
    "plot_x = np.array([np.min(X[:, 2])-1, np.max(X[:, 2])+1])\n",
    "\n",
    "# Từ decision boundary tính x2 = (-1 / θ2) * (θ0 * x1 + θ0)\n",
    "plot_y = (-1 / gradBFGS[2]) * (gradBFGS[1] * plot_x + gradBFGS[0])\n",
    "\n",
    "plt.scatter(45, 85, s=30, c='r', marker='x', label='Ex.1 - 45, Ex.2 - 85')\n",
    "\n",
    "# Plotting the data\n",
    "plotData(X[:,1:], y, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not Admitted')\n",
    "plt.plot(plot_x, plot_y, c='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Với học sinh có điểm Exam 1 là 45 và Exam 2 là 85, bạn có thể thấy xác suất nhập học là 0.776\n",
    "probability = sigmoid(np.dot(gradBFGS, np.array([1, 45.,85.])))\n",
    "\n",
    "print(\"Exam scores: 45 and 85\")\n",
    "print(\"Probability of acceptance: %0.3f\"%(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "Exam scores: 45 and 85\n",
    "Probability of acceptance: 0.776\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bước tiếp theo chúng ta sẽ tính toán độ chính xác của thuật toán (có bao nhiêu giá trị dự đoán đúng).\n",
    "\n",
    "- Khi $h_\\theta(x) \\geq 0.5$ dự đoán $y = 1$.\n",
    "- Khi $h_\\theta(x) < 0.5$ dự đoán $y = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \n",
    "    ## START CODE HERE ### (≈ 1 line of code)\n",
    "    hThetaX = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    arr = []\n",
    "    for h in hThetaX:\n",
    "        \n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        if ( None ):\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return np.array(arr)\n",
    "\n",
    "# Thực hiện dự đoán trên tập dữ liệu training \n",
    "p = predict(gradBFGS, X)\n",
    "\n",
    "# Training accuracy\n",
    "print('Training Accuracy of the classifier: {0}%'.format(np.sum(p==y) / p.size * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng:**\n",
    "\n",
    "```\n",
    "Training Accuracy of the classifier: 89.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tổng kết\n",
    "\n",
    "Thông qua bài tập này, các bạn đã nắm vững các kiến thức về:\n",
    "\n",
    "- Logistic Regression\n",
    "- Triển khai hàm Cost Function và Gradient Method sử dụng BFGS\n",
    "- Đánh giá bài toán Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo\n",
    "\n",
    "[1] [CS229 - Machine Learning]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
