{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !pip install sparktorch\n",
    "# !pip install pyspark\n",
    "# !pip install demoji"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SqbannmbFMd",
    "outputId": "a1a0a2fd-6433-4e40-96e6-685cd7a419f1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from typing import Text\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sparktorch import SparkTorch, serialize_torch_obj\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline, PipelineModel\n",
    "from sparktorch import PysparkPipelineWrapper\n",
    "from pyspark.sql.types import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from Classifier import lstmClassifier\n",
    "from TextTransformer import TextTransformer\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading emoji data ...\n",
      "... OK (Got response in 0.82 seconds)\n",
      "Writing emoji data to C:\\Users\\ASUS\\.demoji\\codes.json ...\n",
      "... OK\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tK3oTr0XaL0p",
    "outputId": "50af4445-06f9-454c-d866-75ce7e5cf855"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "spark = SparkSession.builder \\\r\n",
    "    .appName(\"examples\") \\\r\n",
    "    .master('local[3]')\\\r\n",
    "    .getOrCreate()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "BRsvPUeeapwE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Read in mnist_train.csv dataset\r\n",
    "schema = StructType([StructField('free_text', StringType(), True), StructField('label_id', DoubleType(), True)])\r\n",
    "# df = spark.read.schema(schema).option('header', 'true').csv('uit-hsd/train.csv').repartition(3)\r\n",
    "\r\n",
    "\r\n",
    "train = spark.read.schema(schema)\\\r\n",
    ".option('header', True)\\\r\n",
    ".csv('uit-hsd/train.csv')\r\n",
    "\r\n",
    "test = spark.read.schema(schema)\\\r\n",
    ".option('header', True)\\\r\n",
    ".csv('uit-hsd/test.csv')\r\n",
    "\r\n",
    "\r\n",
    "train = train.where((train.label_id == 0) | (train.label_id == 1) | (train.label_id  == 2))\r\n",
    "\r\n",
    "test = test.where((test.label_id == 0) | (test.label_id == 1) | (test.label_id  == 2))\r\n",
    "\r\n",
    "train = train.withColumn('label_id', when(train.label_id == 2, 1).otherwise(train.label_id))\r\n",
    "test = test.withColumn('label_id', when(test.label_id == 2, 1).otherwise(test.label_id))"
   ],
   "outputs": [],
   "metadata": {
    "id": "Fb__u-0cg8Cu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# over sampling the train set to deal with imbalance class\r\n",
    "new_train = train.withColumn('n',when(train.label_id == 1, 4).otherwise(1))\r\n",
    "new_train = new_train.withColumn('n', expr('explode(array_repeat(n,int(n)))')).select(['free_text', 'label_id'])\r\n",
    "train = new_train"
   ],
   "outputs": [],
   "metadata": {
    "id": "TgBTbXRDM9_B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "vocab = pickle.load(open('/content/vocab.pkl', 'rb'))\r\n",
    "for key in vocab.keys():\r\n",
    "    vocab[key] = int(vocab[key])\r\n",
    "network = lstmClassifier(vocab_size = len(vocab))\r\n",
    "\r\n",
    "# Build the pytorch object\r\n",
    "torch_obj = serialize_torch_obj(\r\n",
    "    model=network,\r\n",
    "    criterion=nn.BCELoss(),\r\n",
    "    optimizer=torch.optim.Adam,\r\n",
    "    lr=0.005\r\n",
    ")\r\n",
    "\r\n",
    "# Setup features\r\n",
    "spark_model = SparkTorch(\r\n",
    "    inputCol='features',\r\n",
    "    labelCol='label_id',\r\n",
    "\r\n",
    "    predictionCol='predictions',\r\n",
    "    torchObj=torch_obj,\r\n",
    "    iters=500,\r\n",
    "    verbose=1,\r\n",
    "    validationPct=0.2,\r\n",
    "    miniBatch=64\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "UZGneVDtbcPT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "my_transformer = TextTransformer(input_col = 'free_text', output_col= 'features')\n",
    "\n",
    "assembler_exploded = VectorAssembler(\n",
    "    inputCols=[\"features[{}]\".format(i) for i in range(50)], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "# Demonstration of some options. Not all are required\n",
    "# Note: This uses the barrier execution mode, which is sensitive to the number of partitions\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "7myPAiLqbcXT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pip = Pipeline(stages = [my_transformer, assembler_exploded, spark_model])\n",
    "pip = pip.fit(train)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Nso76APo23JH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pip.write().overwrite().save('/content/drive/MyDrive/bigdata/lstm_2classes_2')\n",
    "\n",
    "# Example of loading the pipeline\n",
    "loaded_pipeline = PysparkPipelineWrapper.unwrap(PipelineModel.load('/content/drive/MyDrive/bigdata/lstm_2classes_2'))\n",
    "\n",
    "# Run predictions and evaluation\n",
    "predictions = loaded_pipeline.transform(train).persist()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_id\", predictionCol=\"predictions\", metricName=\"accuracy\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "D0TWpNuigtlt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test.numPartitions"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'numPartitions'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8c727a08a1e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1644\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'numPartitions'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "loaded_pipeline = PysparkPipelineWrapper.unwrap(PipelineModel.load('lstm_2classes_2'))\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o103.partitions.\n: java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: (null) entry in command string: null ls -F C:\\Users\\ASUS\\Desktop\\DA\\DA_2classes\\lstm_2classes_2\\metadata\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1733)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1713)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1733)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1713)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:835)\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fbb94ed19d9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded_pipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPysparkPipelineWrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPipelineModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm_2classes_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'language'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mloadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m    523\u001b[0m         \u001b[0mmetadataPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mmetadataStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m         \u001b[0mloadedVals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parseMetaData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataStr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpectedClassName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloadedVals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1531\u001b[0m         \"\"\"\n\u001b[0;32m   1532\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1533\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1534\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o103.partitions.\n: java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: (null) entry in command string: null ls -F C:\\Users\\ASUS\\Desktop\\DA\\DA_2classes\\lstm_2classes_2\\metadata\\part-00000\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1733)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1713)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:835)\r\n\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1733)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1713)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:835)\r\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "predictions1 = predictions.withColumn('predictions', when(predictions.predictions > 0.5,1 )\\\n",
    "                                      .otherwise(0))\n",
    "predictions1 = predictions1.withColumn('predictions', predictions1.predictions.cast(DoubleType()))"
   ],
   "outputs": [],
   "metadata": {
    "id": "5deaO4mCf1yX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "accuracy = evaluator.evaluate(predictions1)\n",
    "print(\"Train accuracy = %g\" % accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy = 0.899992\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2fH2tjHEMjn",
    "outputId": "9270baa6-6686-4b31-f85a-050124ca5946"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "predictions1.groupBy('predictions').count().show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-----+\n",
      "|predictions|count|\n",
      "+-----------+-----+\n",
      "|        0.0|20389|\n",
      "|        1.0|16008|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQnTCVUz_48t",
    "outputId": "a0e5b803-da1c-46e8-a8e6-db24a27fd5dc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "preds = predictions1.toPandas()"
   ],
   "outputs": [],
   "metadata": {
    "id": "7eUg99LOB4Gy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import pandas as pd\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "ub8ZUVUxFYNl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(preds['label_id'].values, preds['predictions'].values))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91     19861\n",
      "         1.0       0.90      0.87      0.89     16536\n",
      "\n",
      "    accuracy                           0.90     36397\n",
      "   macro avg       0.90      0.90      0.90     36397\n",
      "weighted avg       0.90      0.90      0.90     36397\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvKwSXu3e0CW",
    "outputId": "7ab43822-08bb-4414-f961-7f3dcd88202d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "test_preds = loaded_pipeline.transform(test)\n",
    "\n",
    "test_preds = test_preds.withColumn('predictions', when(test_preds.predictions > 0.5,1 )\\\n",
    "                                      .otherwise(0))\n",
    "test_preds = test_preds.withColumn('predictions', test_preds.predictions.cast(DoubleType()))"
   ],
   "outputs": [],
   "metadata": {
    "id": "8_LLDkeufqav"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "test_preds = test_preds.toPandas()"
   ],
   "outputs": [],
   "metadata": {
    "id": "Sapi25Gygn-Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(classification_report(test_preds['label_id'].values, test_preds['predictions'].values))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.88      0.91      5536\n",
      "         1.0       0.54      0.68      0.60      1123\n",
      "\n",
      "    accuracy                           0.85      6659\n",
      "   macro avg       0.74      0.78      0.76      6659\n",
      "weighted avg       0.87      0.85      0.86      6659\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Fj7K5Wcgrn_",
    "outputId": "e729fa7e-04d7-420b-fe3b-08903bc594fa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_preds['predictions'].value_counts()"
   ],
   "outputs": [],
   "metadata": {
    "id": "JCag8F6HsVOT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "preds['predictions'].value_counts()"
   ],
   "outputs": [],
   "metadata": {
    "id": "l-e5IW4ZhLBk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "nQPnxx9csnCA"
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "training_sparktorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}