{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Basics With Numpy - ML / DL\n",
    "\n",
    "Chào mừng các bạn đến với bài tập lập trình về Python cơ bản sử dụng Numpy ứng dụng trong Machine Learning / Deep Learning.\n",
    "\n",
    "Trong bài tập lập trình này, các bạn sẽ ôn lại các chức năng cơ bản của Python đã học trên [AI VIỆT NAM](https://aivietnam.ai/).\n",
    "\n",
    "**Hướng dẫn:**\n",
    "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
    "- Cố gắng không sử dụng các vòng lặp (for, while). \n",
    "- Không chỉnh sửa các tên hàm. \n",
    "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới. \n",
    "\n",
    "**Sau bài tập này bạn sẽ:**\n",
    "- Một số mẹo trên Jupyter Notebook.\n",
    "- Có thể sử dụng các hàm trong thư viện numpy và các phép tính numpy matrix/vector.\n",
    "- Thấy được sự khác nhau giữa thư viện math và thư viện numpy.\n",
    "- Kỹ thuật \"broadcasting\"\n",
    "- Thực hiện tính toán trên các Vector\n",
    "\n",
    "Hãy bắt đầu nào!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hướng dẫn làm bài ##\n",
    "\n",
    "Các bạn sẽ bắt đầu Code trong phần `### START CODE HERE ###` và `### END CODE HERE ###`. Các bạn nhớ đừng sửa bất kỳ tên hàm nào ngoài phần Code này. \n",
    "\n",
    "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó. \n",
    "\n",
    "Trong phần Code: các bạn hãy cố gắng thực hiện ít dòng Code nhất theo chỉ định \"(≈ X lines of code)\". Mặc dù đây không phải là hạn chế về số dòng Code của bạn, nhưng hãy tối ưu sao cho ít nhất có thể.\n",
    "\n",
    "**Bài tập**: Hãy bắt đầu thử dòng Code đầu tiên - thực hiện in ra màn hình dòng chữ `\"Hello World\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "test = None \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**:\n",
    "\n",
    "```python\n",
    "test: Hello World\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Một số điều bạn nên nhớ**:\n",
    "    \n",
    "- Chạy cells bằng SHIFT + ENTER (hoặc nhấn \"Run Cell\")\n",
    "\n",
    "- Chỉ sử dụng Python 3 để code\n",
    "\n",
    "- Không chỉnh sửa bất cứ thứ gì bên ngoài"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Sử dụng Numpy xây dựng 1 hàm cơ bản ##\n",
    "\n",
    "- Trong bài tập này, bạn sẽ thực hiện 1 vài hàm từ thư viện numpy như: np.exp, np.log, np.reshape,... \n",
    "\n",
    "(Truy cập vào trang chủ [numpy](www.numpy.org) để xem chi tiết hơn.)\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "\n",
    "\n",
    "Trước khi sử dụng np.exp(), bạn sẽ sử dụng hàm math.exp() để thực hiện hàm `Sigmoid`. Bạn sẽ thấy lý do tại sao chúng ta sử dụng np.exp() thay cho math.exp().\n",
    "\n",
    "**Bài tập**: Thực hiện xây dựng hàm `Sigmoid` với số thực x. Sử dụng math.exp() cho phần hàm này.\n",
    "\n",
    "**Nhắc lại**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ : là hàm phi tuyến không chỉ được sử dụng nhiều trong Machine Learning mà còn trong Deep Learning.  \n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar \n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = None \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_sigmoid(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "0.9999546021312976\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong thực tế, chúng ta không sử dụng thư viện 'math' trong Machine Learning / Deep Learning (ML/DL) bởi vì đầu vào của hàm này là số thực.\n",
    "Trong ML/DL đầu vào của chúng ta là matrices và vectors. Đó chính là lý do numpy hữu ích hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lý do mà chúng ta sử dung 'numpy' thay cho 'math' trong ML/DL ###\n",
    "x = [1, 2, 3]\n",
    "\n",
    "# Khi chạy Cell này: các bạn sẽ thấy lỗi vì x là vector.\n",
    "basic_sigmoid(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu ta có $ x = (x_1, x_2, ..., x_n)$ là 1 vector hàng thì $np.exp(x)$ sẽ thực hiện tính toán trên từng phần tử của x. Vì vậy, chúng ta sẽ có kết quả như sau: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hơn nữa, nếu x là 1 vector, thì khi thực hiện $s = x + 3$ hay $s = \\frac{1}{x}$ sẽ cho đầu ra cùng kích thước với x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print('x + 3:', x + 3)\n",
    "print('1 / x:', 1/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bất cứ khi nào bạn cần tìm hiểu thêm về các hàm trong Numpy, hãy truy cập vào các [tài liệu chính thức trên trang chủ](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "**Bài tập**: Thực hiện hàm `Sigmoid` sử dụng numpy.\n",
    "\n",
    "**Hướng dẫn**: bây giờ, x có thể là 1 vô hướng, 1 vector hay 1 matrix. Cấu trúc của dạng này là các (vectors, matrices...) được gọi là các numpy array. \n",
    "$$ \\text{Với } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = None \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Đạo hàm của hàm Sigmoid\n",
    "\n",
    "Trong phần [Xây dựng các hàm activation trong machine learning](https://aivietnam.ai/courses/aisummer2019/lessons/ham-activations/) AIVIETNAM đã giải thích chi tiết về phần này. Các bạn có thể xem lại.\n",
    "\n",
    "**Bài tập**: Thực hiện hàm sigmoid_grad() để tính đạo hàm của hàm Sigmoid với đầu vào là x. Công thức toán học: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "**Hướng dẫn**:\n",
    "Bạn sẽ thực hiện 2 bước trong phần này:\n",
    "1. Thực hiện gọi hàm sigmoid với đầu vào là x (Hàm Sigmoid sử dụng numpy mà các bạn đã viết ở trên).\n",
    "2. Tính $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = None\n",
    "    ds = None \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Hai hàm được sử dụng phổ biến trong ML/DL là [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) và [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape: đưa ra số chiều của matrix/vector X.\n",
    "- X.reshape(...): định hình lại X sang một chiều khác.\n",
    "\n",
    "**Câu hỏi phụ**: Các bạn hãy phân biệt sự khác nhau giữa `Transpose` và `Reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print('Matrix A:\\n', A)\n",
    "print('Transpose: \\n', A.T)\n",
    "print('Reshape: \\n', A.reshape(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong Computer Science, hình ảnh được đại diện bởi 1 array 3 chiều có shape $(length, height, depth = 3)$\n",
    "\n",
    "Tuy nhiên, khi bạn thực hiện đưa ảnh vào 1 thuật toán bạn cần chuyển nó thành vector có shape $(length*height*3, 1)$. Nói cách khác là bạn \"unroll\", hay reshape 3D array thành 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Bài tập**: Thực hiện hàm `image2vector()` với đầu vào có shape(length, height, 3) và đầu ra là vector có (length\\*height\\*3, 1). \n",
    "\n",
    "Ví dụ, bạn có 1 array v có shape(a, b, c) -> chúng ta mong muốn đầu ra là 1 vector có shape (a*b, c).\n",
    "\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalizing rows\n",
    "\n",
    "Một kỹ thuật phổ biến khác trong ML / DL là chuẩn hoá dữ liệu. Sau khi chuẩn hoá, chúng ta sẽ có hiệu suất tốt hơn vì Gradient Descent hội tụ nhanh hơn. Ở phần này, chúng ta sẽ chuẩn hoá x thành $ \\frac{x}{\\| x\\|} $ (Chia mỗi vector của x cho Norm tương ứng của nó). \n",
    "\n",
    "Ví dụ, với $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ và $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$thì ta có        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "Lưu ý: bạn có thể nhân, chia ma trận có kích thước khác nhau và nó vẫn hoạt động tốt. Đây là kỹ thuật \"broadcasting\" mà sẽ được tìm hiểu trong phần 1.5. Hoặc bạn có thể xem thêm tại [đây](https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm?fbclid=IwAR0mN3dYIvickJiDwOU7ayjBk5JKNo9wbZWGsyDWMQVA-eQ-Y__3Q4dD-kw)\n",
    "\n",
    "**Bài tập**: Thực hiện hàm normalizeRows() để chuẩn hoá theo hàng của matrix. Sau khi thực hiện hàm này với đầu vào là matrix x bạn sẽ có đầu ra là vector độ dài (có nghĩa là length của vector bằng 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # ord = 2 có nghĩa là trong phần này chúng ta đang input matrix 2D.\n",
    "    \n",
    "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
    "    # print(x_norm.shape)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    \n",
    "    x = x / x_norm\n",
    "    # print(x.shape)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **normalizeRows(x)** </td> \n",
    "       <td> [[ 0.          0.6         0.8       ]\n",
    " [ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chú ý**:\n",
    "\n",
    "Trong normalizeRows(), bạn hãy thử bỏ comment in ra shapes của x_norm và x. Và chạy lại phần này, bạn sẽ thấy chúng khác nhau về số chiều (shape). Mặc dù nó khác nhau số chiều, nhưng vẫn có thể thực hiện tính toán được bình thường, đây gọi là kỹ thuật \"broadcasting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Broadcasting và hàm Softmax ####\n",
    "\n",
    "\n",
    "Một khái niệm quan trọng tỏng numpy là \"broadcasting\". Nó hữu ích cho việc tính toán giữa các arrays với số chiều khác nhau. Để tìm hiểu thêm, bạn có thể đọc chi tiết tại [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "Với kỹ thuật broadcasting, chúng ta có thể dùng phép toán (a+b). Lúc này, vector b sẽ nhân bản ra để có cùng shape với b, và phép toán được thực hiện theo các thông thường.\n",
    "\n",
    "<img src=\"images/broadcasting.png\" style=\"width:800px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập**: Thực hiện hàm softmax sử dụng numpy. Hàm softmax giúp chúng ta phân loại nhiều classes (multi-class). Bạn sẽ được học về softmax trong các tuần tới. \n",
    "\n",
    "**Hướng dẫn**:\n",
    "- $ \\text{Với } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{Matrix } x \\in \\mathbb{R}^{m \\times n}$, $x_{ij}$ ánh xạ tới từng phần từ ở hàng thứ $i^{th}$ cột  $j^{th}$ trong $x$, do đó chúng ta có:\n",
    "\n",
    "$ softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(hàng thứ nhất của x)}  \\\\\n",
    "    softmax\\text{(hàng thứ hai của x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(hàng cuối của x)} \\\\\n",
    "\\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Thực hiện hàm softmax cho mỗi hàng của input x.\n",
    "\n",
    "    Code của bạn sẽ thực hiện trên mỗi hàng của vector.\n",
    "    \n",
    "    Ma trận x có kích thước shape(m ,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Sử dụng exp() element-wise (tính toán trên từng phần tử) cho x. Dùng: np.exp(...).\n",
    "    x_exp = None\n",
    "    print('x_exp - shape:',x_exp.shape)\n",
    "\n",
    "    # Tạo vector x_sum với tổng theo hàng của x_exp. Use: np.sum(..., axis = 1, keepdims = True).\n",
    "    # Đầu vào của chúng ta là matrix 2 chiều nên:\n",
    "    # axis = 0 tương đương với tính tổng theo chiều dọc (cột)\n",
    "    # axis = 1 tương đương với tính tổng theo chiều ngang (hàng)\n",
    "    # keepdims = True -> giữ số chiều giống đầu vào.  \n",
    "    \n",
    "    x_sum = None\n",
    "    print('x_sum - shape', x_sum.shape)\n",
    "    \n",
    "    # Thực hiện hàm softmax(x) bằng phép chia x_exp cho x_sum. Bạn nên sử dụng kỹ thuật 'broadcasting'.\n",
    "    s = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))\n",
    "\n",
    "y = np.array([[9], \n",
    "              [5]])\n",
    "print(y.shape)\n",
    "print('x / y: ', x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**:\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> **softmax(x)** </td> \n",
    "       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
    "    1.21052389e-04]\n",
    " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
    "    8.01252314e-04]]</td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chú ý**:\n",
    "\n",
    "- Nếu bạn in ra số chiều của x_exp, x_sum. Bạn sẽ thấy x_sum có shape(2, 1) trong khi đó x_exp và s có shape (2, 5). **x_exp/x_sum** hoạt động được do 'broadcasting'.\n",
    "\n",
    "\n",
    "Chúc mừng bạn đã hoàn thành phần Code bên trên về python numpy và kỹ thuật 'broadcasting'. Nó là các hàm hữu ích trong ML / DL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Tổng kết một số điều bạn nên nhớ:**\n",
    "    \n",
    "- np.exp(x) hoạt động trên từng phần tử của np.array x và thực hiện hàm e mũ trên từng phần tử trong x\n",
    "\n",
    "- hàm sigmoid và đạo hàm của nó \n",
    "\n",
    "- chuyển hình ảnh 3D sang vector (image2vector), rất hữu ích trong deep learning\n",
    "\n",
    "- thực hiện chuyển đổi số chiều với np.reshape \n",
    "\n",
    "- kỹ thuật 'broadcasting' rất hiệu quả trong tính toán "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ML / DL, bạn sẽ sử dụng với những bộ dữ liệu lớn. Vì thế mà chúng ta cần phải sử dụng thuật toán tối ưu để tính toán, tránh lãng phí thời gian. Bạn nên vector hoá (vectorization) sẽ giúp tính toán hiệu quả hơn. \n",
    "\n",
    "Một vài ví dụ bên dưới, bạn sẽ thấy sự khác biệt giữa phép toán dot / outer / elementwise product sử dụng vectorization và vòng lặp for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù với bộ dữ liệu nhỏ như ví dụ ở trên, bạn mới chỉ thấy thời gian chênh lệch không đáng kể. Nhưng với dữ liệu lớn, nó sẽ rất lãng phí thời gian chạy thuật toán của bạn.\n",
    "\n",
    "\n",
    "**Lưu ý**: hàm `np.dot()` thực hiện phép nhân matrix-matrix hoặc matrix_vector. Nó khác với phép `np.multiply()` và `*`. ( `np.multiply()` và `*` có nghĩa là nhân từng phần tử element-wise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Thực hiện tính toán hàm Loss L1 và L2 \n",
    "\n",
    "**Bài tập**: Thực hiện tính L1 loss với numpy. \n",
    "\n",
    "**Nhắc lại**:\n",
    "- Hàm loss sẽ đánh giá hiệu suất của model. Nếu loss lớn điều đó có nghĩa là bạn giá trị dự đoán ($ \\hat{y} $) sai khác lớn so với giá trị thực tế ($y$).\n",
    "- L1 loss được định nghĩa như sau:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L1** </td> \n",
    "       <td> 1.1 </td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập**: Thực hiện tính toán hàm L2 loss. \n",
    "\n",
    "Có một cách khác để thực hiện hàm L2 loss là bạn sử dụng np.dot(). \n",
    "\n",
    "Với: $x = [x_1, x_2, ..., x_n]$, thì `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- Công thức hàm L2 Loss: $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    # Using function np.dot() useful.\n",
    "    loss = None \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúc mừng bạn đã hoàn thành bài tập này. Mong rằng nó sẽ giúp ích cho các bạn !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Những điều bạn cần nhớ:**\n",
    "    \n",
    "- Vectorization rất quan trong trong Deep Learning. Nó giúp tính toán hiệu quả hơn. \n",
    "\n",
    "- Xem nhớ và phân biệt hàm loss: l1 và l2\n",
    "\n",
    "- Các hàm: np.sum, np.dot, np.multiply, np.maximun, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo\n",
    "[1] [Deep Learning - Coursera](https://coursera.org)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
