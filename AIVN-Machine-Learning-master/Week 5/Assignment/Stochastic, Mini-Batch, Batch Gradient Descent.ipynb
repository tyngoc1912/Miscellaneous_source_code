{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment: Stochastic, Mini-batch, Batch Gradient Descent\n",
    "\n",
    "Chào mừng các bạn đến với bài tập lập trình về Stochastic, Mini-batch, Batch Gradient Descent. Trước khi thực hiện bài tập này, các bạn nên học kỹ các kiến thức lý thuyết. Nếu có bất kỳ câu hỏi hay vấn đề nào xảy ra, các bạn hãy để lại comment trực tiếp bên dưới bài đăng hoặc liên hệ qua Fanpage AIVIETNAM.\n",
    "\n",
    "### Hướng dẫn làm bài \n",
    "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
    "- Cố gắng không sử dụng các vòng lặp (for, while). \n",
    "- Hãy sử dụng các hàm của thư viện numpy.\n",
    "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới. \n",
    "\n",
    "Các bạn sẽ bắt đầu Code trong phần `### START CODE HERE ###` và `### END CODE HERE ###`. Các bạn nhớ đừng sửa bất kỳ dòng Code nào bên ngoài những câu lệnh này. \n",
    "\n",
    "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó. \n",
    "\n",
    "Trong phần Code: các bạn hãy cố gắng thực hiện ít dòng Code nhất theo chỉ định \"(≈ X lines of code)\". Mặc dù đây không phải là hạn chế về số dòng Code của bạn, nhưng hãy tối ưu sao cho ít nhất có thể.\n",
    "\n",
    "### Nhiệm vụ \n",
    "\n",
    "Trong bài tập này, các bạn sẽ đánh giá sự khác nhau giữa các biến thể của Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Xây dựng bộ dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Tạo dữ liệu m = 8000\n",
    "mean = np.array([5.0, 6.0]) \n",
    "cov = np.array([[1.0, 0.95], [0.95, 1.2]]) \n",
    "data = np.random.multivariate_normal(mean, cov, 8000) \n",
    "  \n",
    "# Trực quan hoá 500 dữ liệu ban đầu \n",
    "plt.scatter(data[:500, 0], data[:500, 1], marker = '.') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm cột dữ liệu 1 vào data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((np.ones((data.shape[0], 1)), data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chia dữ liệu thành tập Train và tập Test\n",
    "```\n",
    "Tỷ lệ: split_factor = 0.90 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_factor = 0.90\n",
    "split = int(split_factor * data.shape[0]) \n",
    "  \n",
    "X_train = data[:split, :-1] \n",
    "y_train = data[:split, -1].reshape((-1, 1)) \n",
    "X_test = data[split:, :-1] \n",
    "y_test = data[split:, -1].reshape((-1, 1)) \n",
    "  \n",
    "print(\"Số lượng dữ liệu trong tập Train = % d\"%(X_train.shape[0])) \n",
    "print(\"Số lượng dữ liệu trong tập Test = % d\"%(X_test.shape[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Xây dựng thuật toán Linear Regression \n",
    "\n",
    "#### 1. Hàm giả thuyết\n",
    "\n",
    ">$h_{\\theta}(X) = X\\theta$\n",
    "\n",
    "#### 2. Cost Function\n",
    "\n",
    "> $J(\\theta) = \\dfrac{1}{2m} (X\\theta - \\vec{y} )^T(X\\theta - \\vec{y})$\n",
    "\n",
    "#### 3. Gradient\n",
    "\n",
    "> $grad = X^T (X\\theta - \\vec{y}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm giả thuyết \n",
    "def hypothesis(X, theta): \n",
    "    return np.dot(X, theta) \n",
    "\n",
    "# Hàm mất mát \n",
    "def cost(X, y, theta): \n",
    "    h = hypothesis(X, theta) \n",
    "    J = np.dot((h - y).T, (h - y)) \n",
    "    J /= 2\n",
    "    return J[0] \n",
    "\n",
    "# Gradient \n",
    "def gradient(X, y, theta): \n",
    "    h = hypothesis(X, theta) \n",
    "    grad = np.dot(X.T, (h - y)) \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Descent \n",
    "\n",
    "**Bài tập:** Xây dựng Gradient Descent \n",
    "\n",
    "Trong phần này, chúng ta sẽ xây dựng thuật toán Gradient Descent:\n",
    "\n",
    "> $\\theta:= \\theta - \\frac{\\alpha}{m} * grad $\n",
    "\n",
    "Với `minibatch_size` tuỳ chọn ta có thể tạo thành 3 loại:\n",
    "```\n",
    "minibatch_size = 1: Stochastic Gradient Descent \n",
    "minibatch_size = m (số lượng train): Batch Gradient Descent \n",
    "minibatch_size = k (1 < k < m): Mini-batch Gradient Descent \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def gradient_descent(X, y, learning_rate,  minibatch_size, n_iterations = 50):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    theta = np.zeros((X.shape[1], 1)) \n",
    "    losses = []\n",
    "    \n",
    "    m_train = y.shape[0]\n",
    "    \n",
    "    for epoch in range(n_iterations):\n",
    "        \n",
    "        ### START CODE HERE ### (≈ 3 line of code)\n",
    "        \"\"\"\n",
    "        Xáo trộn bộ dữ liệu sử dụng hàm: numpy.random.permutation()\n",
    "        \"\"\"\n",
    "        shuffled_indices = None\n",
    "        X_shuffled = None\n",
    "        y_shuffled = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        for i in range(0, m_train, minibatch_size):\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 3 line of code)\n",
    "            \"\"\"\n",
    "            Chia dữ liệu sau khi xáo trộn vào các batch \n",
    "            X_shuffled[i:i+minibatch_size]\n",
    "            \"\"\"\n",
    "            xi = None\n",
    "            yi = None\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            m = yi.shape[0]\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 1 line of code)\n",
    "            theta = None\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            losses.append(cost(xi, yi, theta))\n",
    "            \n",
    "    print(\"--- Thời gian chạy: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập:** Gọi hàm `gradient_descent` đã xây dựng và thực hiện so sánh giữa các thuật toán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stochastic Gradient Descent \n",
    "\n",
    "```\n",
    "learning_rate = 0.001\n",
    "minibatch_size = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "theta, losses = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"Bias = \", theta[0]) \n",
    "print(\"Coefficients = \", theta[1:]) \n",
    "print(\"Loss: \", losses[-1])\n",
    "  \n",
    "# visualising gradient descent \n",
    "plt.plot(losses) \n",
    "plt.xlabel(\"Number of iterations\") \n",
    "plt.ylabel(\"Cost\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting output for X_test \n",
    "y_pred = hypothesis(X_test, theta) \n",
    "plt.scatter(X_test[:, 1], y_test[:, ], marker = '.') \n",
    "plt.plot(X_test[:, 1], y_pred, color = 'orange') \n",
    "plt.show() \n",
    "  \n",
    "# calculating error in predictions \n",
    "error = np.sum(np.abs(y_test - y_pred) / y_test.shape[0]) \n",
    "print(\"Mean absolute error = \", error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mini-batch Gradient Descent \n",
    "\n",
    "```\n",
    "learning_rate = 0.001\n",
    "minibatch_size = k (k tuỳ chọn: thường là 8, 16, 32, 64, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "theta, losses = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"Bias = \", theta[0]) \n",
    "print(\"Coefficients = \", theta[1:]) \n",
    "print(\"Loss: \", losses[-1])\n",
    "  \n",
    "# visualising gradient descent \n",
    "plt.plot(losses) \n",
    "plt.xlabel(\"Number of iterations\") \n",
    "plt.ylabel(\"Cost\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting output for X_test \n",
    "y_pred = hypothesis(X_test, theta) \n",
    "plt.scatter(X_test[:, 1], y_test[:, ], marker = '.') \n",
    "plt.plot(X_test[:, 1], y_pred, color = 'orange') \n",
    "plt.show() \n",
    "  \n",
    "# calculating error in predictions \n",
    "error = np.sum(np.abs(y_test - y_pred) / y_test.shape[0]) \n",
    "print(\"Mean absolute error = \", error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Gradient Descent \n",
    "\n",
    "```\n",
    "learning_rate = 0.001\n",
    "minibatch_size = m (số lượng train)\n",
    "```\n",
    "Nhận xét: với `learning_rate = 0.001` giống với 2 thuật toán bên trên. Các bạn sẽ thấy thuật toán không tối ưu. Hãy lựa chọn `learning_rate` khác tối ưu hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "theta, losses = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"Bias = \", theta[0]) \n",
    "print(\"Coefficients = \", theta[1:]) \n",
    "print(\"Loss: \", losses[-1])\n",
    "  \n",
    "# visualising gradient descent \n",
    "plt.plot(losses) \n",
    "plt.xlabel(\"Number of iterations\") \n",
    "plt.ylabel(\"Cost\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting output for X_test \n",
    "y_pred = hypothesis(X_test, theta) \n",
    "plt.scatter(X_test[:, 1], y_test[:, ], marker = '.') \n",
    "plt.plot(X_test[:, 1], y_pred, color = 'orange') \n",
    "plt.show() \n",
    "  \n",
    "# calculating error in predictions \n",
    "error = np.sum(np.abs(y_test - y_pred) / y_test.shape[0]) \n",
    "print(\"Mean absolute error = \", error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tổng kết\n",
    "\n",
    "Thông qua bài tập này, các bạn đã nắm thấy rõ được sự khác nhau giữa các thuật toán:\n",
    "- Với mỗi thuật toán khác nhau, việc lựa chọn Learning Rate có ảnh hưởng đến việc hội tụ của thuật toán.\n",
    "- Thời gian chạy thuật toán.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
