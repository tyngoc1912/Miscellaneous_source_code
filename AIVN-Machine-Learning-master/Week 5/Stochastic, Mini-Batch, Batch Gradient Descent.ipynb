{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nhắc lại về Gradient Descent \n",
    "\n",
    "Gradient Descent là một thuật toán lặp tối ưu hoá được sử dụng trong các bài toán Machine Learning / Deep Learning (thường là các bài toán tối ưu lồi — Convex Optimization).\n",
    "\n",
    "<img src=\"images/gd-2.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "Cách mà thuật toán Gradient Descent thực hiện là lấy đạo hàm của Cost Function theo $\\theta$ (hay còn gọi là Weight - trọng số). Chúng ta đang cố gắng đưa chấm đen trên hình càng gần về điểm Minimum càng tốt bằng cách di chuyển **ngược dấu** với đạo hàm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Batch Gradient Descent\n",
    "\n",
    "Thuật toán Gradient Descent từ đầu cho đến bài này chúng ta đều sử dụng Batch Gradient Descent. Có nghĩa là: chúng ta đang sử dụng **toàn bộ dữ liệu** của tập training khi cập nhật tham số $\\theta$. Nhược điểm của phương pháp này là với bộ dữ liệu lớn (hàng triệu dữ liệu) thì mỗi lần thực hiện tính toán đạo hàm tại mỗi vòng lặp sẽ rất tốn thời gian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ: Xét bài toán Linear Regression với $m$ dữ liệu training.\n",
    "\n",
    "<img src=\"images/gd/1.png\" style=\"width:70%;height:70%;\">\n",
    "\n",
    "Bây giờ thuật toán Batch Gradient Descent của chúng ta sẽ sử dụng toàn bộ tập dữ liệu.\n",
    "\n",
    "<img src=\"images/gd/2.png\" style=\"width:70%;height:70%;\">\n",
    "\n",
    "Nếu chúng ta có m = 3 triệu dữ liệu thì mỗi lần cập nhật $\\theta$ sẽ phải tính 3 triệu lần hoặc sử dụng Vector hoá nhưng cũng gây tốn thời gian tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mã code Gradient Descent\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "       Thực hiện Gradient để tìm theta\n",
    "    \"\"\"\n",
    "    m = y.size  # số lượng training\n",
    "    for epoch in range(num_iters):\n",
    "        y_hat = np.dot(X, theta)\n",
    "        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stochastic Gradient Descent \n",
    "\n",
    "Trong Stochastic Gradient Descent (SGD), tại mỗi vòng lặp ta chỉ tính đạo hàm của hàm mất mát dựa trên **chỉ một** điểm dữ liệu $x_i$ rồi cập nhật $\\theta$ dựa trên đạo hàm này. Chú ý hàm mất mát ở đây được lấy trung bình trên mỗi điểm dữ liệu nên đạo hàm tại một điểm cũng được kỳ vọng là khá gần với đạo hàm của hàm mất mát trên mọi điểm dữ liệu.\n",
    "\n",
    "<img src=\"images/gd/3.png\" style=\"width:70%;height:70%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mã giả thực hiện SGD \n",
    "def SGD(X, y, theta, alpha, num_iters):\n",
    "    m = y.size\n",
    "    for epoch in range(num_iters):\n",
    "        for i in range(m):\n",
    "            # lấy ngẫu nhiên 1 sample\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            \n",
    "            \"\"\"\n",
    "            Thực hiện tính SGD cho xi, yi.\n",
    "            Chúng ta sẽ thực hành chi tiết về SGD trong phần bài tập.\n",
    "            \"\"\"\n",
    "       \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mini-batch Gradient Descent \n",
    "\n",
    "Đúng theo cái tên của nó là Mini-batch tức chúng ta sẽ lấy một số lượng $k$ dữ liệu training với $1 < k < m$. Ta sẽ chia mỗi mini-batch có $k$ dữ liệu, có trường hợp mini-batch cuối cùng sẽ ít hơn $k$ nếu $m$ không chia hết cho $k$.\n",
    "\n",
    "Chú ý rằng trong mỗi Epoch chúng ta sẽ xáo trộn dữ liệu ngẫu nhiên rồi chia vào mỗi mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mã giả thực hiện Mini-batch GD \n",
    "\n",
    "def mini_batch_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "\n",
    "    m = y.size\n",
    "    minibatch_size = 5\n",
    "    \n",
    "    for epoch in range(num_iters):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_b_shuffled = X_b[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "                \n",
    "        for i in range(0, m, minibatch_size):\n",
    "            xi = X_b_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            \n",
    "            \"\"\"\n",
    "            Thực hiện cập nhật theta với Mini-batch size = 5\n",
    "            Chúng ta sẽ thực hành chi tiết về Mini-batch GD trong phần bài tập.\n",
    "            \"\"\"\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. So sánh tốc độ hội tụ\n",
    "\n",
    "Vì SGD sử dụng từng mẫu đơn cho tốc độ tính toán đạo hàm nhanh hơn nhưng tốc độ hội tụ của nó lâu hơn so với Mini-batch GD.\n",
    "\n",
    "<img src=\"images/gd/gd_sgd.png\" style=\"width:70%;height:70%;\">\n",
    "\n",
    "Mini-batch GD thường được sử dụng hơn vì:\n",
    "- Tốc độ hội tụ nhanh hơn Stochastic Gradient Descent.\n",
    "- Tốc độ tính đoạ hàm nhanh hơn Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo \n",
    "\n",
    "[1] [CS230 - Deep Learning]()\n",
    "\n",
    "[2] [Biến thể của Gradient Descent - Machine Learning Cơ Bản](https://machinelearningcoban.com/2017/01/16/gradientdescent2/#-batch-gradient-descent)\n",
    "\n",
    "[3] [Difference between Batch Gradient Descent and Stochastic Gradient Descent](https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
