{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "training_mlmodel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install pyspark\r\n",
        "# !pip install demoji"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 56 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=ca651891a93a3430e2500eda7916062380393a5d40b8a3285d95b792addb6c3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CamkHclb3Q_g",
        "outputId": "b586e88d-7f61-4acc-8ab7-65ef45ac1883"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import VectorAssembler\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "from pyspark.ml.pipeline import Pipeline, PipelineModel\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "from pyspark.sql import DataFrame\r\n",
        "from pyspark import keyword_only\r\n",
        "from pyspark.ml import Transformer\r\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\r\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "# import demoji\r\n",
        "\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "0x-3GDU03c1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class TextTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\r\n",
        "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\r\n",
        "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\r\n",
        "\r\n",
        "    @keyword_only\r\n",
        "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\", ):\r\n",
        "        super(TextTransformer, self).__init__()\r\n",
        "        self._setDefault(input_col=None, output_col=None)\r\n",
        "        kwargs = self._input_kwargs\r\n",
        "        self.set_params(**kwargs)\r\n",
        "        \r\n",
        "\r\n",
        "    @keyword_only\r\n",
        "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\r\n",
        "        kwargs = self._input_kwargs\r\n",
        "        self._set(**kwargs)\r\n",
        "\r\n",
        "    def get_input_col(self):\r\n",
        "        return self.getOrDefault(self.input_col)\r\n",
        "\r\n",
        "    def get_output_col(self):\r\n",
        "        return self.getOrDefault(self.output_col)\r\n",
        "\r\n",
        "\r\n",
        "    def _transform(self, df: DataFrame):\r\n",
        "        def preprocess_text(text, ) -> str:\r\n",
        "            # try:\r\n",
        "            #     text = demoji.replace_with_desc(str(text), sep = '', )\r\n",
        "            # except:\r\n",
        "            #     text = ''\r\n",
        "            text = re.sub(r'\\d+', '', str(text)).translate(str.maketrans( string.punctuation, ' '*len(string.punctuation)),).strip().lower()\r\n",
        "            return text\r\n",
        "        input_col = self.get_input_col()\r\n",
        "        output_col = self.get_output_col()\r\n",
        "        # The custom action: concatenate the integer form of the doubles from the Vector\r\n",
        "        transform_udf = udf(preprocess_text, StringType())\r\n",
        "        new_df = df.withColumn(output_col, transform_udf(input_col))\r\n",
        "        return new_df\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "2IH1WOaoGx7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "spark = SparkSession.builder\\\r\n",
        "    .appName(\"examples\")\\\r\n",
        "    .master('local[3]')\\\r\n",
        "    .getOrCreate()\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "gpLvJ_9mAdDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Read in mnist_train.csv dataset\r\n",
        "schema = StructType([StructField('free_text', StringType(), True), StructField('label_id', DoubleType(), True)])\r\n",
        "# df = spark.read.schema(schema).option('header', 'true').csv('/content/drive/MyDrive/bigdata/uit-hsd/train.csv').repartition(3)\r\n",
        "\r\n",
        "\r\n",
        "train = spark.read.schema(schema)\\\r\n",
        ".option('header', True)\\\r\n",
        ".csv('/content/drive/MyDrive/bigdata/uit-hsd/train.csv')\r\n",
        "\r\n",
        "test = spark.read.schema(schema)\\\r\n",
        ".option('header', True)\\\r\n",
        ".csv('/content/drive/MyDrive/bigdata/uit-hsd/test.csv')\r\n",
        "\r\n",
        "\r\n",
        "train = train.where((train.label_id == 0) | (train.label_id == 1) | (train.label_id  == 2))\r\n",
        "\r\n",
        "test = test.where((test.label_id == 0) | (test.label_id == 1) | (test.label_id  == 2))\r\n",
        "\r\n",
        "train = train.withColumn('label_id', when(train.label_id == 2, 1).otherwise(train.label_id))\r\n",
        "test = test.withColumn('label_id', when(test.label_id == 2, 1).otherwise(test.label_id))"
      ],
      "outputs": [],
      "metadata": {
        "id": "3A3NGqQQAoML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# over sampling the train set to deal with imbalance class\n",
        "new_train = train.withColumn('n',when(train.label_id == 1, 4).otherwise(1))\n",
        "new_train = new_train.withColumn('n', expr('explode(array_repeat(n,int(n)))')).select(['free_text', 'label_id'])\n",
        "train = new_train"
      ],
      "outputs": [],
      "metadata": {
        "id": "cJcI_cQvArAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
        "from pyspark.ml.feature import IDF, StringIndexer, StopWordsRemover, CountVectorizer, RegexTokenizer\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol=\"label_id\", outputCol=\"label\")\n",
        "text_transformer = TextTransformer(input_col='free_text', output_col='free_text')\n",
        "regex_tokenizer = RegexTokenizer(inputCol='free_text', outputCol=\"words\", pattern=\"[^0-9a-z#+_]+\")\n",
        "\n",
        "count_vectorizer = CountVectorizer(inputCol='words', outputCol=\"countFeatures\", minDF=5)\n",
        "idf = IDF(inputCol='countFeatures', outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol='features', labelCol=\"label\", )\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "                            # label_stringIdx,\n",
        "    text_transformer,\n",
        "    regex_tokenizer,\n",
        "    count_vectorizer,\n",
        "    idf,\n",
        "    lr])"
      ],
      "outputs": [],
      "metadata": {
        "id": "eL-luf2XC76j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "label_fitted = label_stringIdx.fit(train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VySj3Y7yOmxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "train = label_fitted.transform(train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nkHpixivOywI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "model = pipeline.fit(train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DjHewUQgL4e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "model.save('text_classifier')"
      ],
      "outputs": [],
      "metadata": {
        "id": "lFntMytHRPki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "loaded_model = PipelineModel.load('text_classifier')"
      ],
      "outputs": [],
      "metadata": {
        "id": "QKhAtMjDPBlp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "predictions = loaded_model.transform(train).toPandas()"
      ],
      "outputs": [],
      "metadata": {
        "id": "28rmH8eMMvDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(predictions['label'].values, predictions['prediction'].values))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.87      0.82     19861\n",
            "         1.0       0.81      0.70      0.75     16536\n",
            "\n",
            "    accuracy                           0.79     36397\n",
            "   macro avg       0.80      0.78      0.79     36397\n",
            "weighted avg       0.79      0.79      0.79     36397\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E7YLzzCPvyP",
        "outputId": "845c371a-4541-456c-a425-99c31730ceff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "test_predictions = loaded_model.transform(test).toPandas()"
      ],
      "outputs": [],
      "metadata": {
        "id": "FCxzo_C_QaBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_predictions['label_id'].values, test_predictions['prediction'].values))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.86      0.88      5536\n",
            "         1.0       0.44      0.56      0.49      1123\n",
            "\n",
            "    accuracy                           0.81      6659\n",
            "   macro avg       0.67      0.71      0.69      6659\n",
            "weighted avg       0.83      0.81      0.81      6659\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubGwhEcKQfnP",
        "outputId": "7398ea2d-66f5-4b3e-b768-54f937ea8581"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "CtLlyWJnPtWS"
      }
    }
  ]
}