{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài trước, chúng ta đã cùng tìm hiểu về bài toán [Hồi quy tuyến tính](https://nbviewer.jupyter.org/github/thanhhff/AIVN-Machine-Learning/blob/master/Week%202/Linear-Regression-with-One-Variable.ipynb) - trong bài toán hồi quy, đầu ra của bài toán là dữ liệu liên tục. Trong phần này, chúng ta tìm hiểu về bài toán học có giám sát - Hồi quy Logistic với đầu ra là dữ liệu rời rạc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Liệu có thể sử dụng Linear Regression cho bài toán Logistic Regression?\n",
    "\n",
    "Đặt vấn đề: Tại sao bài toán hồi quy tuyến tính (Linear Regression) lại không thể sử dụng cho bài toán Logistic Regression?\n",
    "\n",
    "Giả sử trong trường hợp chúng ta cần phân loại một người có mắc bệnh ung thư (1) hay không mắc bệnh ung thư (0).\n",
    "\n",
    "Trong thuật toán của Linear Regression không đưa ra xác suất mắc bệnh để dự đoán là 0 hay 1 mà sẽ tính khoảng cách từ các điểm dữ liệu đến siêu mặt phẳng sao cho khoảng cách này là nhỏ nhất - tức hàm Loss đạt giá trị thấp nhất có thể:\n",
    "\n",
    "<br><center>$J(\\theta) = \\dfrac{1}{2m}  \\sum_{i = 1}^{m} {(h_{\\theta}(x^{(i)}) - y^{(i)})^2}$</center>\n",
    "\n",
    "Chúng ta sẽ sử dụng kích thước khối u (Tumor Size) để dự đoán. Kết quả của bài toán sẽ được như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.stack.imgur.com/VVtRW.png\" style=\"width:30%;height:30%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong hình trên: 1 - ung thư, 0 - không ung thư và đường kẻ màu xanh là hàm giả thuyết $h(x)$. Có thể dễ dàng phân loại được với dữ liệu như này: \n",
    "- Nếu $h(x) >= 0.5$: kết luận người này bị ung thư.\n",
    "- Nếu $h(x) < 0.5$: kết luận không bị ung thư."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên, thật may mắn với các dữ liệu ở trên. Bây giờ chúng ta sẽ thêm dữ liệu mới và xem kết quả dự đoán: \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/nEC4H.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "Lần này, nếu $h(x) >= 0.5$ không thể phân loại tốt được như trước nữa. Mà phải thay đổi hàm $h(x) >= 0.2$ để có thể dự đoán chính xác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta không thể thực hiện thay đổi giá trị hàm giả thuyết $h(x)$ mỗi khi có dữ liệu thay đổi. Mà thay vào đó, chúng ta sẽ sử dụng bài toán hồi quy Logistic để phân loại 2 nhóm (phân loại nhị phân 0 hay 1). Trong các bài tới, chúng ta sẽ tìm hiểu về bài toán phân loại nhiều nhóm đối tượng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bài toán Logistic Regression \n",
    "\n",
    "#### 2.1 Xây dựng hàm giả thuyết $h_{\\theta}(x)$\n",
    "\n",
    "Chúng ta mong muốn giá trị đầu ra: $0 \\leq  h_{\\theta}(x) \\leq  1$. Chính vì vậy mà chúng ta sử dụng một Activation Function (hàm kích hoạt) để thay đổi thành hàm phi tuyến.\n",
    "\n",
    "Đầu ra dự đoán của Logistic Regression thường được ký hiệu: \n",
    "\n",
    "<center> $h_{\\theta}(x) = g(\\theta^Tx)$ <br> $z = \\theta^Tx$ </center>\n",
    "\n",
    "Trong đó: \n",
    "> $g$: kí hiệu cho Activation Function \n",
    "\n",
    "Một số Activation Function: \n",
    "\n",
    "<img src=\"images/ActivationFunction.png\" style=\"width:60%;height:60%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong các Activation Fuction bên trên, hàm `sigmoid` được sử dụng cho bài toán Logistic Regression vì nó bị chặn trong khoảng [0, 1] - thoả mãn bài toán phân loại nhị phân:\n",
    "\n",
    "- Khi $h_\\theta(x) \\geq 0.5$ dự đoán $y = 1$.\n",
    "- Khi $h_\\theta(x) < 0.5$ dự đoán $y = 0$.\n",
    "\n",
    "**Một số tính chất:**\n",
    "\n",
    "> $h_{\\theta}(x) = P(y=1|x;\\theta) = 1 - P(y=0|x;\\theta)$<br><br>\n",
    "> $P(y=1|x;\\theta) + P(y=0|x;\\theta) = 1$\n",
    "\n",
    "Ví dụ: Nếu xác suất là 1 là 70% thì xác suất là 0 là 30%.\n",
    "\n",
    "<img src=\"images/chap7-logistic_6.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "**Nhắc lại hàm Sigmoid:**\n",
    "\n",
    "Xem chi tiết về phân tích hàm Sigmoid [tại đây](https://nbviewer.jupyter.org/github/thanhhff/AIVN-Machine-Learning/blob/master/Week%205/activation-function.ipynb).\n",
    "\n",
    "> Công thức: $f(t) = \\frac{1}{1 + e^{-t}}$ <br><br>\n",
    "> Đạo hàm: $f'(t) = \\frac{e^{-t}}{(1 + e^{-t})^2} = \\frac{1}{1 + e^{-t}} \\frac{e^{-t}}{1 + e^{-t}} = f(t)(1 - f(t))$\n",
    "\n",
    "<img src=\"images/sigmoid_function.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "**Khi đó hàm giả thuyết:**\n",
    "\n",
    "> $h_{\\theta}(x) = g(\\theta^Tx)$ <br><br>\n",
    "> $z = \\theta^Tx$ <br><br>\n",
    "> $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Các bạn hãy thử thay đổi các giá trị của $\\theta$ để hiểu rõ hơn về Sigmoid Function tại (https://www.desmos.com/calculator/bgontvxotm).\n",
    "\n",
    "<img src=\"images/sigmoid-change.png\" style=\"width:50%;height:50%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Decision Boundary (Đường ranh giới)\n",
    "\n",
    "Decision Boundary là đường phân chia ranh giới giữa các nhóm (có thể là đường thẳng hoặc không).\n",
    "\n",
    "- Khi $h_\\theta(x) \\geq 0.5$ dự đoán $y = 1$.\n",
    "- Khi $h_\\theta(x) < 0.5$ dự đoán $y = 0$.\n",
    "\n",
    "<img src=\"images/decision-boundary.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "\n",
    "Ta có:\n",
    "\n",
    "> $z = 0, e^0 = 1 \\Rightarrow g(z) = 1/2$ <br><br>\n",
    "> $z \\rightarrow \\infty, e^{-\\infty} \\rightarrow 0 \\Rightarrow g(z) = 1$ <br><br>\n",
    "> $z \\rightarrow -\\infty, e^{\\infty} \\rightarrow \\infty \\Rightarrow g(z) = 0$\n",
    "\n",
    "Suy ra ta có: \n",
    "\n",
    "> $h_\\theta(x) = g(\\theta^Tx) \\geq 0.5$ khi $z \\geq 0 \\Leftrightarrow  \\theta^Tx \\geq 0$\n",
    "\n",
    "Từ những chứng minh trên, đường ranh giới được đưa ra bởi quyết định sau:\n",
    "\n",
    "> $\\theta^Tx \\geq 0 \\Rightarrow y = 1$<br><br>\n",
    "> $\\theta^Tx < 0 \\Rightarrow y = 0$\n",
    "\n",
    "**Ví dụ:**\n",
    "\n",
    "Với $h_{\\theta}(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)$ \n",
    "\n",
    "<img src=\"images/decision-boundary-1.png\" style=\"width:20%;height:20%;\">\n",
    "\n",
    "Giả sử: $\\theta^T = [-3, 1, 1]$\n",
    "\n",
    "Dự đoán $y = 1$ khi: \n",
    "> $-3 + x_1 + x_2 \\geq 0$ <br><br>\n",
    "> $\\Rightarrow x_1 + x_2 \\geq 3$\n",
    "\n",
    "Nếu vẽ đường thẳng $x_1 + x_2 = 3$ chúng ta sẽ có đường ranh giới giữa các nhóm.\n",
    "\n",
    "<img src=\"images/decision-boundary-2.png\" style=\"width:20%;height:20%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost Function \n",
    "\n",
    "Xét tập $m$ dữ liệu training: $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... , (x^{(m)}, y^{(m)})\\}$\n",
    "\n",
    "$x \\in \\begin{bmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "\\cdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}, R^{m+1}, x_0 = 1$; $y \\in (0, 1)$\n",
    "\n",
    "Ta có:\n",
    "\n",
    "> $h_\\theta(x_0) = \\frac{1}{1 + e^{-\\theta^Tx}}$\n",
    "\n",
    "Ta cần giải quyết bài toán tối ưu tìm $\\theta$ tương tự bài toán maximum likelihood.\n",
    "\n",
    "Giả sử:\n",
    "\n",
    "> $P(y = 1 | x; \\theta) = h_\\theta(x)$ <br><br>\n",
    "> $P(y = 0 | x; \\theta) = 1 - h_\\theta(x)$\n",
    "\n",
    "Có thể viết gọn thành:\n",
    "\n",
    "> $p(y | x; \\theta) = (h_\\theta(x))^y (1 - h_\\theta(x))^{1-y}$\n",
    "\n",
    "Giải thích:\n",
    "- Khi $y = 0$ thì: $p(y | x; \\theta) = (h_\\theta(x))^0 (1 - h_\\theta(x))^{1-0} = 1 - h_\\theta(x)$\n",
    "- Khi $y = 1$ thì: $p(y | x; \\theta) = (h_\\theta(x))^1 (1 - h_\\theta(x))^{1-1} = h_\\theta(x)$\n",
    "\n",
    "Với tập $m$ dữ liệu training, ta có thể viết:\n",
    "\n",
    "> $p(\\vec{y} | X; \\theta) = \\prod_{i = 1}^{m} p(y^{(i)} | x^{(i)}; \\theta) = \\prod_{i = 1}^{m}(h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)})^{1-y^{(i)}}$ \n",
    "\n",
    "Chúng ta mong muốn tìm giá trị nhỏ nhất của $J(\\theta)$ nên thực hiện lấy logarit tự nhiên, đổi dấu (để lấy min) và lấy trung bình của hàm số ta thu được:\n",
    "\n",
    "> $J(\\theta) = - \\frac{1}{m} [ \\sum_{i = 1}^{m}  y^{(i)}\\log{h(x^{(i)})} +  (1 - y^{(i)})\\log{h(1 - x^{(i)})} ]$\n",
    "\n",
    "**Khai triển bằng Vector:**\n",
    "\n",
    "> $h = g(X\\theta)$ <br><br>\n",
    "> $J(\\theta) = - \\frac{1}{m}[y^T \\log{h} + (1-y)^T\\log{(1 - h)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Descent \n",
    "\n",
    "**Chú ý: Công thức cuối cùng sau khi lấy đạo hàm theo $\\theta$ giống với Linear Regression. Chứng minh chi tiết bên dưới dành cho bạn nào muốn tìm hiểu chi tiết.**\n",
    "\n",
    "> Thực hiện đạo hàm $J$ lần lượt theo các tham số $\\theta_0, \\theta_1 ... \\theta_n$ .\n",
    "> Lặp lại cho đến khi hội tụ:\n",
    ">\n",
    "> {\n",
    ">\n",
    "> $\\theta_0 := \\theta_0 - \\alpha \\dfrac{1}{m} \\sum_{i=1}^m {(h_{\\theta}(x^{(i)}) - y^{(i)})}.x_0^{(i)} $\n",
    ">\n",
    "> $\\theta_1 := \\theta_1 - \\alpha \\dfrac{1}{m} \\sum_{i=1}^m {((h_{\\theta}(x^{(i)}) -y^{(i)})}.x_1^{(i)} $\n",
    ">\n",
    "> $\\vdots$ \n",
    ">\n",
    "> $\\theta_n := \\theta_n - \\alpha \\dfrac{1}{m} \\sum_{i=1}^m {((h_{\\theta}(x^{(i)}) -y^{(i)})}.x_n^{(i)} $\n",
    "> \n",
    "> }\n",
    "\n",
    "**Biểu diễn bằng Vector:**\n",
    "\n",
    "> $\\theta:= \\theta - \\frac{\\alpha}{m} X^T (g(X\\theta) - \\vec{y}) $\n",
    "\n",
    "**Chứng minh (Tham khảo):**\n",
    "\n",
    "Thực hiện lấy đạo hàm $J$ theo $\\theta$:\n",
    "\n",
    "<img src=\"images/gd.png\" style=\"width:70%;height:70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced Optimization\n",
    "\n",
    "Ngoài việc sử dụng thuật toán Gradient Descent thông thường chúng ta còn có nhiều thuật toán khác giúp tìm giá trị tối ưu của $\\theta$ sao cho $J(\\theta)$ nhỏ nhất.\n",
    "\n",
    "Một số thuật toán như: \"[Conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\", \"[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\", \"[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\",... sẽ giúp bạn tìm giá trị hội tụ nhanh hơn. Tuy nhiên, trong bài viết này không đi chi tiết vào bên trong từng thuật toán mà chúng ta sẽ sử dụng các thư viện có sẵn để tránh phức tạp và lỗi phát sinh. \n",
    "\n",
    "Ưu điểm của phương pháp này:\n",
    "- Không cần phải chọn learning_rate (có một vòng lặp bên trong thuật toán để tìm ra giá trị learning_rate tốt nhất).\n",
    "- Thường hội tụ nhanh hơn Gradient Descent.\n",
    "- Dễ dàng sử dụng mà không cần tìm hiểu quá sâu bên trong.\n",
    "\n",
    "Nhược điểm:\n",
    "- Không nên tự thực hiện mà hãy sử dụng thư viện có sẵn.\n",
    "- Nếu có lỗi xảy ra việc sửa lỗi thường phức tạp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tổng kết \n",
    "\n",
    "1. Hàm giả thuyết\n",
    "\n",
    "> $h_{\\theta}(x) = g(\\theta^Tx)$ <br><br>\n",
    "> $z = \\theta^Tx$ <br><br>\n",
    "> $g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "2. Cost Function\n",
    "\n",
    "> $h = g(X\\theta)$ <br><br>\n",
    "> $J(\\theta) = - \\frac{1}{m}[y^T \\log{h} + (1-y)^T\\log{(1 - h)}]$\n",
    "\n",
    "3. Gradient Descent \n",
    "\n",
    "> $\\theta:= \\theta - \\frac{\\alpha}{m} X^T (g(X\\theta) - \\vec{y}) $\n",
    "\n",
    "4. Advanced Optimization\n",
    "\n",
    "Một số thuật toán tối ưu khác: \"[Conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\", \"[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\", \"[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\",..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo \n",
    "\n",
    "[1] [CS229 - Machine Learning](https://www.coursera.org/learn/machine-learning/resources/Zi29t)\n",
    "\n",
    "[2] [Linear vs. Logistic Regression on Classification Problems](https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
