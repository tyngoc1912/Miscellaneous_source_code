{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient Descent Tips \n",
    "\n",
    "*Nhắc lại*: Công thức cập nhật $\\theta$ ở vòng lặp thứ $t$:\n",
    "<center>$\\theta_{t+1} := \\theta_t - \\alpha \\Delta_{\\theta} f(\\theta_t)$</center>\n",
    "\n",
    "Trong đó:\n",
    "- $\\alpha$: learning rate - tốc độ học tập.\n",
    "- $\\Delta_{\\theta} f(\\theta_t)$: đạo hàm của hàm số tại điểm $\\theta$.\n",
    "\n",
    "Việc lựa chọn giá trị $\\alpha$ (learning rate) rất quan trọng. Nó quyết định việc bài toán có thể hội tụ tới giá trị global minimum cho hàm $f(\\theta)$ hay không. Gradient Descent có thể làm việc hiệu quả hơn bằng cách chọn Learning Rate phù hợp.\n",
    "\n",
    "Một số trường hợp về lựa chọn Learning Rate như sau (các bạn có thể thử thay đổi [tại đây](https://developers.google.com/machine-learning/crash-course/fitter/graph)):\n",
    "\n",
    "**Learning Rate quá lớn** - Gradient Descent không thể hội tụ được về giá trị Minimum.\n",
    "\n",
    "<img src=\"images/image-3.gif\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "**Learning Rate quá nhỏ**: - Gradient Descent có thể hội tụ được về giá trị Minimum trong bài toán này, nhưng mất tới 81 vòng lặp để hội tụ. Trong một số bài toán có nhiều giá trị cực tiểu địa phương - LR quá nhỏ có thể khiến hàm số bị mắc kẹt tại cực tiểu địa phương và không bao giờ hội tụ được về giá trị tối ưu.\n",
    "\n",
    "<img src=\"images/image-5.png\" style=\"width:50%;height:50%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate vừa:** nếu Learning Rate quá nhỏ khiến bài toán hội tụ lâu, các bạn hãy thử tăng giá trị này lên thêm. Trong bài toán này khi Learning Rate = 1.0 sẽ mất 6 vòng lặp để hội tụ.\n",
    "\n",
    "<img src=\"images/image-4.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "**Learning Rate tối ưu:** trong thực tế rất khó có thể tìm ra được giá trị Learning Rate tối ưu. Việc tìm được giá trị Learning Rate tương đối với giá trị tối ưu sẽ giúp bài toán hội tụ nhanh hơn.\n",
    "\n",
    "<img src=\"images/image-6.png\" style=\"width:50%;height:50%;\">\n",
    "\n",
    "\n",
    "**Tổng kết:** \n",
    "- Nếu Learning Rate quá nhỏ: mất quá nhiều thời gian để hội tụ, đồng thời có thể bị mắc kẹt ở cực tiểu địa phương.\n",
    "- Nếu Learning Rate quá lớn: không thể hội tụ được."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Một vài lời khuyên \n",
    "\n",
    "- Trước khi bắt đầu bài toán, các bạn hãy chuẩn hoá dữ liệu về khoảng [-1;1] hay [0;1] sẽ giúp bài toán hội tụ nhanh hơn.\n",
    "- Bắt đầu bài toán bằng Learning Rate nhỏ. Tăng dần Learning Rate nếu không thấy phù hợp.\n",
    "- Với bài toán nhiều dữ liệu hãy sử dụng Mini-batch Gradient Descent (phương pháp này sẽ được đề cập trong bài tới).\n",
    "- Sử dụng Momentum cho Gradient Descent (phương pháp này sẽ được đề cập trong bài tới)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normal Equation\n",
    "\n",
    "Normal Equation là phương pháp tìm nghiệm của bài toán Linear Regression mà không cần tới vòng lặp, không cần lựa chọn Learning Rate. Và cũng không cần phải Scaling dữ liệu.\n",
    "\n",
    "Công thức toán đằng sau nghiệm của phương trình này các bạn có thể đọc thêm tại:\n",
    "\n",
    "https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression\n",
    "\n",
    "Và công thức quan trọng nhất của chúng ta:\n",
    "\n",
    "<center> $\\theta = (X^T X)^{-1} X^Ty $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sánh giữa Normal Equation và Gradient Descent:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> Gradient Descent </td>\n",
    "    <td> Normal Equation </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> Cần phải chọn Learning Rate </td>\n",
    "    <td> Không cần chọn Learning Rate </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> Cần nhiều vòng lặp </td>\n",
    "    <td> Không cần vòng lặp </td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> Thời gian tính: $O(kn^2)$ </td>\n",
    "    <td> Thời gian tính: $O(n^3)$, cần phải tính ma trận nghịch đảo </td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td> Hoạt động tốt với dữ liệu lớn  </td>\n",
    "    <td> Rất chậm với dữ liệu lớn </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với Normal Equation, việc tính toán mất thời gian $O(n^3)$ nên với dữ liệu lớn (n > 10.000 dữ liệu) chúng ta nên sử dụng Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo \n",
    "\n",
    "[1] [CS229 - Machine Learning Course](http://cs229.stanford.edu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
