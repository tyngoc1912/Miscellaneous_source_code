{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cây quyết định\n",
    "**Cây quyết định (Decision Tree)** là một phương pháp học máy có giám sát không tham số được sử dụng để phân lớp và hồi quy.\n",
    "\n",
    "Mục đích của cây quyết định là tạo ra một mô hình dự đoán kết quả mục tiêu bằng cách học các luật quyết định đơn giản được suy diễn ra từ các đặc trưng dữ liệu.\n",
    "\n",
    "Mỗi tập luật định nghĩa ra một giả thuyết, có thể được biểu diễn bằng một cây quyết định với đường đi xuôi từ gốc đến lá cho ta một luật quyết định. Nút gốc và mỗi nút trên cây là một thuộc tính/ điều kiện kiểm tra, các nhánh đi xuống từ nút ứng với các giá trị có thể của thuộc tính/điều kiện này. Nhãn của các mẫu phù hợp là các nút lá.\n",
    "\n",
    "\n",
    "Hình dưới đây minh họa một cây quyết định của dữ liệu **Titanic** dự đoán khả năng sống sót khi tàu chìm.\n",
    "<img src=\"titanic.png\" style=\"text-align:center; max-height:400px\">\n",
    "\n",
    "**Bài tập:** Mô tả tập luật của cây quyết định trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trả lời**: *Điền đáp án vào đây!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình cây quyết định trong Scikit-learn\n",
    "Trong `Scikit-learn`, mô hình cây quyết định được cài đặt trong gói `tree` với `DecisionTreeClassifier`.\n",
    "\n",
    "**Bài tập:** Import dữ liệu và mô hình cây quyết định từ `Scikit-learn`, sau đó huấn luyện và biểu diễn mô hình thu được sau khi huấn luyện.\n",
    "\n",
    "*Gợi ý:* Sử dụng kiến thức từ bài thực hành trước với mô hình `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import mô hình và dữ liệu cần thiết từ thư viện\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "import pandas as pdx\n",
    "\n",
    "# TODO: Chia dữ liệu huấn luyện và kiểm tra hợp lý\n",
    "\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# TODO: Huấn luyện mô hình với dữ liệu\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Visualize mô hình vừa được xây dựng với tập dữ liệu kiểm tra\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các thuật toán xây dựng cây quyết định cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vấn đề cơ bản của bài toán xây dựng cây quyết định là:\n",
    "- Xác định thuộc tính/điều kiện của mỗi nút\n",
    "- Thứ tự các nút\n",
    "\n",
    "Trong bài này, chúng ta sẽ làm quen với 2 thuật toán cơ bản nhất là **ID3** (Iterative Dichotomiser 3) và **C4.5** (Classification and Regression Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thuật toán ID3 (Quinlan 1986) chọn thuộc tính tốt nhất của tập huấn luyện được làm nút gốc theo tiêu chuẩn cực đại lượng thu hoạch thông tin (Information Gain). \n",
    "\n",
    "### Entropy\n",
    "Entropy dùng để đo độ không chắc chắn (độ mù mờ của thông tin). Nếu ta tập dữ liệu $D$ có $N$ phần tử, thuộc $C$ lớp và số phần tử mỗi lớp là $N_c$ thì entropy của tập dữ liệu $D$ được tính theo công thức:\n",
    "\n",
    "$$ E(D) = - \\sum_{c=1}^{C} \\frac{N_c}{N}\\log (\\frac{N_c}{N}) = - \\sum_{c=1}^{C}p_c\\log(p_c)$$\n",
    "**Bài tập**: Định nghĩa hàm `entropy(freq)` để tính entropy của phân phối xác suất dữ liệu `freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 2.5764258916820024\n"
     ]
    }
   ],
   "source": [
    "# TODO: Để có thể xây dựng được cây quyết định, việc đầu tiên cần làm là tính \n",
    "#       toán entropy cho dữ liệu với một phân phối cho trước (hoặc được tính\n",
    "#       toán thông qua dữ liệu)\n",
    "#       Định nghĩa hàm entropy(freq) tính toán độ mù mờ của dữ liệu với phân \n",
    "#       phối xác suất freq, là tần suất của mỗi lớp c trong bộ dữ liệu D. Hàm trả về số thực là độ đo entropy tương ứng.\n",
    "import numpy as np\n",
    "def entropy(freq):\n",
    "    return -np.sum(np.log2(freq) * freq)\n",
    "\n",
    "freq = np.array([0.2, 0.3, 0.12, 0.18, 0.08, 0.06, 0.06])\n",
    "print(\"Entropy = {}\".format(entropy(freq)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy hai thuộc tính\n",
    "Khi thuộc tính $x_i$ được chọn làm nút, chia tập $D$ thành $K$ nhánh con $D_1, D_2,...,D_k$, số lượng phần tử trong mỗi nốt con kí hiệu là $m_k$. Độ đo entropy  sau phép chia này được tính:\n",
    "$$ E(D,x_i) = \\sum_{k=1}^{K} \\frac{m_k}{N}E(D_k)= \\sum_{k\\in K}P(k)E(D_k) $$\n",
    "\n",
    "**Bài tập:** Tính độ đo entropy khi có thêm một thuộc tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0, 0, 1, 0])\n",
      "dict_values([0, 2, 1, 0])\n",
      "dict_values([4, 0, 0, 0])\n",
      "dict_values([5, 1, 0, 0])\n",
      "dict_values([0, 2, 5, 0])\n",
      "dict_values([0, 1, 0, 0])\n",
      "dict_values([0, 1, 2, 0])\n",
      "dict_values([1, 3, 3, 0])\n",
      "dict_values([0, 2, 2, 0])\n",
      "dict_values([8, 2, 0, 0])\n",
      "dict_values([0, 0, 1, 0])\n",
      "dict_values([2, 5, 1, 0])\n",
      "dict_values([2, 0, 0, 0])\n",
      "dict_values([0, 0, 3, 0])\n",
      "dict_values([0, 1, 3, 0])\n",
      "dict_values([0, 0, 1, 0])\n",
      "dict_values([1, 0, 0, 0])\n",
      "dict_values([8, 1, 0, 0])\n",
      "dict_values([5, 0, 0, 0])\n",
      "dict_values([2, 5, 0, 0])\n",
      "dict_values([0, 1, 4, 0])\n",
      "dict_values([0, 5, 1, 0])\n",
      "dict_values([0, 3, 6, 0])\n",
      "dict_values([3, 1, 0, 0])\n",
      "dict_values([4, 1, 1, 0])\n",
      "dict_values([0, 0, 4, 0])\n",
      "dict_values([0, 0, 1, 0])\n",
      "dict_values([0, 4, 2, 0])\n",
      "dict_values([0, 4, 2, 0])\n",
      "dict_values([1, 0, 0, 0])\n",
      "dict_values([1, 0, 0, 0])\n",
      "dict_values([0, 2, 0, 0])\n",
      "dict_values([0, 0, 1, 0])\n",
      "dict_values([0, 3, 5, 0])\n",
      "dict_values([3, 0, 0, 0])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Khi chọn thêm một thuộc tính làm nốt chia, ta phải tính entropy với\n",
    "#       thuộc tính mới để tìm ra thuộc tính chia tốt nhất.\n",
    "#       Định nghĩa hàm _entropy(data, prev_attr, target_attr):\n",
    "#       - data (np.array): các mẫu huấn luyện\n",
    "#       - prev_attr(id): thuộc tính chia được chọn trước đó. Nếu không có thuộc\n",
    "#       tính nào được chọn trước đó, prev_attr = -1\n",
    "#       - target_attr(id): thuộc tính chia cần tính entropy\n",
    "# Gợi ý: Sử dụng lại hàm entropy()\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "\n",
    "def _entropy(data, target, target_attr):\n",
    "    N = len(data)\n",
    "    D = Counter(data[:,target_attr]).items()\n",
    "    for D, m in D:\n",
    "        freq = Counter(target[data[:,target_attr] == D])\n",
    "        for i in range(4):\n",
    "            if i not in freq:\n",
    "                freq[i] = 0\n",
    "#         freq = map(lambda x: x)\n",
    "        print(freq.values(), sum(freq.values))\n",
    "    return 0\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "data, target = dataset['data'], dataset['target']    \n",
    "print(_entropy(data, target, 0))\n",
    "# # Tính entropy cho dữ liệu hoa cẩm chướng\n",
    "# first_entropy = _entropy(data, -1, 0)\n",
    "# print(\"First attribute (without prev_attr): {}\".format(first_entropy))\n",
    "# # Giả sử tập dữ liệu đã được chia bởi thuộc tính đầu tiên: Độ dài của lá\n",
    "# second_entropy = _entropy(data, 0, 1)\n",
    "# print(\"Second pivot attribute: {}\".format(second_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Độ thu hoạch thông tin\n",
    "Độ thu hoạch thông tin được tính là độ giảm entropy khi biết thêm một thông tin $x$:\n",
    "$$ Gain(D,x_i) = G(D,x_i)= E(D) - E(D,x_i) $$\n",
    "\n",
    "Thuộc tính nào cho độ mù mờ thông tin (entropy) nhỏ nhất hay có độ thu hoạch thông tin lớn nhất sẽ được chọn làm thuộc tính tại nút.\n",
    "\n",
    "$$ x^* = \\underset{x}{\\arg\\max}G(D,x_i) = \\underset{x}{\\arg\\min}E(D,x_i) $$\n",
    "**Bài tập:** Viết hàm tính độ thu hoạch thông tin khi thử chọn một thuộc tính làm thuộc tính chia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Dựa vào công thức ở trên, định nghĩa hàm gain(data, new_attr) tính độ\n",
    "#       thu hoạch thông tin khi chia nhỏ tập dữ liệu theo thuộc tính mới.\n",
    "def gain(data, new_attr):\n",
    "    # Tính entropy của tập dữ liệu\n",
    "    data_entropy = None\n",
    "    \n",
    "    # Tính entropy khi tập dữ liệu bị chia bởi thuộc tính mới\n",
    "    # Khi chọn thuộc tính lần thứ nhất, thuộc tính chia được chọn trước đó có id = -1\n",
    "    data_entropy_divide = None\n",
    "    \n",
    "    # Tính độ thu hoạch thông tin\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "Thuật toán C4.5 được đề xuất năm 1993 bởi Quinlan nhằm khắc phục điểm yếu của thuật toán ID3: áp dụng Tỷ lệ thu hoạch thông tin cực đại (Gain Ratio).\n",
    "\n",
    "Tỷ lệ thu hoạch này phạt các thuộc tính có nhiều giá trị bằng cách thêm vào một hạng tử gọi là `thông tin chia` (Split Information), đại lượng này rất nhạy cảm với việc đánh giá tính rộng và đồng nhất khi chia tách dữ liệu theo giá trị thuộc tính:\n",
    "$$ SplitInformation(D,x_i)=-\\sum_{i=1}^{k} \\frac{\\left|D_i\\right|}{\\left|D\\right|} \\log{\\frac{\\left|D_i\\right|}{\\left|D\\right|}}$$\n",
    "`Split Information` thực tế là entropy của tập dữ liệu `D` ứng với thuộc tính chia `x_i`.\n",
    "\n",
    "Khi đó, tỷ lệ thông tin chia được tính bằng cách chia độ thu hoạch thông tin cho thông tin chia.\n",
    "$$ GainRatio(D, x_i) = \\frac{Gain(D,x_i)}{SplitInformation(D,x_i)} $$\n",
    "\n",
    "**Bài tập:** Hoàn thành hàm `split_infor()` tính thông tin chia và hàm `gain_ratio()` để tỷ lệ thu hoạch thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa hai hàm split_infor() và gain_ratio() để cải thiện thuật toán \n",
    "#       ID3 theo ý tưởng của C4.5\n",
    "def split_infor(data, new_attr):\n",
    "    pass\n",
    "\n",
    "def gain_ratio(data, new_attr):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
